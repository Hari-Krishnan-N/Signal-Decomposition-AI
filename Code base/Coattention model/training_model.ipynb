{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('E:/Amrita/Subjects/Sem 5/BMSP paper work/Code base/Trainer')\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from trainer import ModularTrainer\n",
    "from dataset import get_data_loaders\n",
    "from model import CoattentionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CoattentionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmdb_path = \"E:/Amrita/Subjects/Sem 5/BMSP paper work/Dataset/Final VMD/VMD.lmdb\"\n",
    "seed = 42\n",
    "train_loader, val_loader = get_data_loaders(lmdb_path=lmdb_path, batch_size=16, num_workers=12, prefetch_factor=2, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 2e-4\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ModularTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,  \n",
    "    val_loader=val_loader,\n",
    "    criterion=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    config={'epochs': 1,\n",
    "            'save_dir': 'E:/Amrita/Subjects/Sem 5/BMSP paper work/Code base/Coattention model/Checkpoints',\n",
    "            'log_dir': 'E:/Amrita/Subjects/Sem 5/BMSP paper work/Code base/Coattention model/Logs'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = trainer.train()\n",
    "\n",
    "# plt.plot(history['train_loss'], label='Training Loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Training Loss Over Epochs')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = trainer.train()\n",
    "\n",
    "plt.plot(history['train_loss'], label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('E:/Amrita/Subjects/Sem 5/BMSP paper work/Code base/Coattention model/Graphs/training_loss.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TorchEnv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
