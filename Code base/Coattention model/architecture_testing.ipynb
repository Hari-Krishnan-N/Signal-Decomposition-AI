{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('E:/Deep learning assets (XXXXXXXX)/Classification trainer')\n",
    "sys.path.append('E:/Amrita/Subjects/Sem 5/BMSP paper work/Code base/Trainer')\n",
    "\n",
    "import torch\n",
    "from torchinfo import summary \n",
    "from dataset3 import get_data_loaders, DecompositionDataset\n",
    "from model import CoattentionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lmdb_path = \"E:/Amrita/Subjects/Sem 5/BMSP paper work/Dataset/Final VMD/VMD.lmdb\"\n",
    "# seed = 42\n",
    "# train_loader, test_loader = get_data_loaders(lmdb_path=lmdb_path, batch_size=5, num_workers=12, prefetch_factor=2, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_to_lmdb('LMDB files/output.lmdb', 'Dataset/Sample', 'Dataset/audio_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DecompositionDataset(lmdb_path='E:/Amrita/Subjects/Sem 5/BMSP paper work/Dataset/Final VMD/VMD.lmdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data[0]['IMF'].shape, data[0]['Spectrogram'].shape, data[0]['Label'].shape, data[0]['Image Mask'].shape, data[0]['Audio Mask'].shape\n",
    "data[0]['Reconstructed'].shape, data[0]['Log Spectrogram'].shape, data[0]['Mel Spectrogram'].shape, data[0]['Label'].shape, data[0]['Image Mask'].shape, data[0]['Audio Mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]['Image Mask'], data[0]['Audio Mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.set_printoptions(profile=\"full\")\n",
    "\n",
    "for i in data[0]['IMF']:\n",
    "    print(i)\n",
    "    \n",
    "#torch.set_printoptions(profile=\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "\n",
    "# processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "# model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CoattentionModel()\n",
    "\n",
    "img = torch.rand(4, 10, 512, 512)\n",
    "audio = torch.rand(4, 1440, 1024)\n",
    "img_mask = torch.randint(0, 2, (4,640)).type(torch.float32)\n",
    "audio_mask = torch.randint(0, 2, (4,1440)).type(torch.float32)\n",
    "\n",
    "summary(model=model,\n",
    "        input_data=(img, audio, img_mask, audio_mask),\n",
    "        col_names=['input_size', 'output_size', 'num_params', 'trainable'],\n",
    "        col_width=20,\n",
    "        row_settings=['var_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForSequenceClassification\n",
    "from torchinfo import summary\n",
    "from torch import nn\n",
    "\n",
    "wave2vec = Wav2Vec2ForSequenceClassification.from_pretrained(\"facebook/wav2vec2-base\", num_labels=1024, output_hidden_states=True)\n",
    "\n",
    "summary(wave2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ganes\\anaconda3\\envs\\TorchEnv2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\ganes\\anaconda3\\envs\\TorchEnv2\\Lib\\site-packages\\transformers\\configuration_utils.py:311: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "================================================================================================================================================================\n",
       "Layer (type (var_name))                                                          Input Shape          Output Shape         Param #              Trainable\n",
       "================================================================================================================================================================\n",
       "CoattentionModel (CoattentionModel)                                              [4, 8, 512, 512]     [4, 1]               --                   True\n",
       "├─ImageEncoder (img_encoder)                                                     [4, 8, 512, 512]     [4, 512, 1024]       --                   True\n",
       "│    └─Sequential (patch_conv)                                                   [4, 8, 512, 512]     [4, 8, 256, 256]     --                   True\n",
       "│    │    └─Conv2d (0)                                                           [4, 8, 512, 512]     [4, 8, 256, 256]     1,032                True\n",
       "│    │    └─ReLU (1)                                                             [4, 8, 256, 256]     [4, 8, 256, 256]     --                   --\n",
       "│    │    └─BatchNorm2d (2)                                                      [4, 8, 256, 256]     [4, 8, 256, 256]     16                   True\n",
       "│    └─Dropout (dropout)                                                         [4, 8, 256, 256]     [4, 8, 256, 256]     --                   --\n",
       "│    └─TransformerEncoder (encoder)                                              [4, 512, 1024]       [4, 512, 1024]       --                   True\n",
       "│    │    └─ModuleList (layers)                                                  --                   --                   18,912,256           True\n",
       "├─Dropout (dropout)                                                              [4, 512, 1024]       [4, 512, 1024]       --                   --\n",
       "├─AudioEncoder (audio_encoder)                                                   [4, 147456]          [4, 1, 1024]         --                   True\n",
       "│    └─Wav2Vec2ForSequenceClassification (wave2vec)                              [4, 147456]          [4, 460, 768]        --                   True\n",
       "│    │    └─Wav2Vec2Model (wav2vec2)                                             [4, 147456]          [4, 460, 768]        94,371,712           True\n",
       "│    │    └─Linear (projector)                                                   [4, 460, 768]        [4, 460, 256]        196,864              True\n",
       "│    │    └─Linear (classifier)                                                  [4, 256]             [4, 1024]            263,168              True\n",
       "├─Dropout (dropout)                                                              [4, 1, 1024]         [4, 1, 1024]         --                   --\n",
       "├─TransformerEncoder (final_encoder)                                             [4, 513, 1024]       [4, 513, 1024]       --                   True\n",
       "│    └─ModuleList (layers)                                                       --                   --                   --                   True\n",
       "│    │    └─TransformerEncoderLayer (0)                                          [4, 513, 1024]       [4, 513, 1024]       4,728,064            True\n",
       "│    │    └─TransformerEncoderLayer (1)                                          [4, 513, 1024]       [4, 513, 1024]       4,728,064            True\n",
       "│    │    └─TransformerEncoderLayer (2)                                          [4, 513, 1024]       [4, 513, 1024]       4,728,064            True\n",
       "│    │    └─TransformerEncoderLayer (3)                                          [4, 513, 1024]       [4, 513, 1024]       4,728,064            True\n",
       "│    │    └─TransformerEncoderLayer (4)                                          [4, 513, 1024]       [4, 513, 1024]       4,728,064            True\n",
       "│    │    └─TransformerEncoderLayer (5)                                          [4, 513, 1024]       [4, 513, 1024]       4,728,064            True\n",
       "├─Sequential (final_fc)                                                          [4, 1024]            [4, 1]               --                   True\n",
       "│    └─Linear (0)                                                                [4, 1024]            [4, 256]             262,400              True\n",
       "│    └─ReLU (1)                                                                  [4, 256]             [4, 256]             --                   --\n",
       "│    └─Linear (2)                                                                [4, 256]             [4, 1]               257                  True\n",
       "================================================================================================================================================================\n",
       "Total params: 142,376,089\n",
       "Trainable params: 142,376,089\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 91.09\n",
       "================================================================================================================================================================\n",
       "Input size (MB): 35.92\n",
       "Forward/backward pass size (MB): 3547.50\n",
       "Params size (MB): 382.69\n",
       "Estimated Total Size (MB): 3966.11\n",
       "================================================================================================================================================================"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchinfo import summary\n",
    "from model2 import CoattentionModel\n",
    "\n",
    "model = CoattentionModel()\n",
    "\n",
    "img = torch.rand(4, 8, 512, 512)\n",
    "audio = torch.rand(4, 147456)\n",
    "img_mask = torch.randint(0, 2, (4,512)).type(torch.float32)\n",
    "\n",
    "summary(model=model,\n",
    "        input_data=(img, audio, img_mask),\n",
    "        col_names=['input_size', 'output_size', 'num_params', 'trainable'],\n",
    "        col_width=20,\n",
    "        row_settings=['var_names'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TorchEnv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
